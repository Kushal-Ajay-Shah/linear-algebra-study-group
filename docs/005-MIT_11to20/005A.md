# 11 - Bases of new vector spaces, Rank one matrices

**All for 3x3 cases**
* Basis - 9 dimensional (1 at each cell)
* Basis of symmetric matrices - 6 dimensional
* Basis of upper triangular matrices - 6 dimensional
* Basis of (Symmetric ∩ Upper triangular) - 3 dimensional (since diagonal)
* Basis of (Symmetric ∪ Upper triangular) - 9 dimensional (since all 3x3)

* Every rank 1 matrix can be expressed as - u x v<sup>T</sup>
  * where `u` and `v` are column matrices

* Let M be all 5x17 matrices with rank 4. Is M a subspace?
  * No. Since no `0` matrix


# 12 - Graphs and Networks, Incidence matrices, Kirchoff's Law

Consider this directed graph - 

* Here `m` = 5 edges, `n` = 4 nodes
* `A` - Incidence matrix can be used to denote this graph

* Let Null space solution of `A` be `x` - 

* `Ax` denotes the Potential Difference between nodes
  * shows circuit movement only when potential difference

* Let `C` be a matrix which takes us from Potential Difference to Current through edges (Ohm's law)
* `C` - Conductance matrix
* Let current be `Y` - 

* A<sup>T</sup>y = 0 - Solving Left null space (Kirchoff's current law)
  * Solving left null space just means current coming `IN` a node is equal to current going `OUT` through it
  * can find basis by assuming current (`y`) through one edge and solving such that no charge accumulation
  * Dimension of Left Null Space = Number of independent loops
  * Rank = Number of nodes - 1
  * dim(N(A<sup>T</sup>)) = m - r
  * #loops = #edges - (#nodes - 1)
  * #nodes - #edges + #loops = 1 --> `Euler's Formula`


# 13 - Quiz Review



# 14 - Orthogonal vectors and subspaces

* `x` and `y` are orthogonal if - x<sup>T</sup>y = 0
* Two subspaces are orthogonal if every vector in subspace 1 is perpendicular to every other vector in subspace 2
* row space is perpendicular to null space
* column space is perpendicular to left null space


# 15 - Projections, Least squares, Projection matrix

* Why projections?
  * Because `Ax = b` may not have a solution
  * projection means changing `b` to closest vector in column space of `A`
  * Ax<sup>^</sup> = p

### Projection for line

[Diagram]

* a ⊥ e
* a<sup>T</sup>e = 0
* a<sup>T</sup>(b - p) = 0
* a<sup>T</sup>(b - xa) = 0
* x.a<sup>T</sup>.a = a<sup>T</sup>b
* **x = (a<sup>T</sup>b) / (a<sup>T</sup>a)** - scalar
* **p = ax = a ((a<sup>T</sup>b) / (a<sup>T</sup>a))** - vector (closest `b`)

* Since projection of `b`, `p` is governed by `b` i.e scaling `b` also scales `p`
* So we can write `p` as:
  * **p = (a a<sup>T</sup> / (a<sup>T</sup>a)) b**
  * here (a a<sup>T</sup> / (a<sup>T</sup>a)) is known as Projection matrix (`P`)

* Column space of `P` - line `a`
* rank(P) = 1
* P = P<sup>T</sup> .... symmetrical
* P<sup>2</sup> = P .... projecting same line twice, thrice will not change projection

### Projection for matrix 

[Diagram]

* a<sub>1</sub><sup>T</sup>e = 0 = a<sub>2</sub><sup>T</sup>e
* e = b - p = b - Ax<sup>^</sup>
* a<sub>1</sub><sup>T</sup>(b - Ax<sup>^</sup>) = 0 = a<sub>2</sub><sup>T</sup>(b - Ax<sup>^</sup>)
* Combining both equations (writing them in matrix form)
  * A<sup>T</sup>(b - Ax<sup>^</sup>) = 0
  * A<sup>T</sup>Ax<sup>^</sup> = A<sup>T</sup>b

* **x = (A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>b**
* **p = A(A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>b**
* Here projection matrix `P` is A(A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>

* If `A` is square - `P = I` .... since column space is whole space
* P = P<sup>T</sup> .... symmetrical
* P<sup>2</sup> = P .... projecting same line twice, thrice will not change projection

### Application - least sqaure fitting by a line

* fit points on a straight line
